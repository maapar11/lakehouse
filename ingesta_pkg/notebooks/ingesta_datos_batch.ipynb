{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ece19a-c130-4cf0-ba67-e70d68746145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Integración de Datos Batch\n",
    "\n",
    "Este cuaderno implementa el Motor de Ingesta Batch.\n",
    "\n",
    "Realiza una lectura incremental desde Landing con Databricks Auto Loader. Además, archiva ficheros procesados de Landing a Raw.\\ \n",
    "Es capaz de detectar todos los formatos de ficheros (csv, avro, parquet...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7717c4a-a860-4e6f-bc38-f6e9d91f4cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Primero, vamos a definir las variables base de los path de las zonas involucradas en la ingesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c73e8a2-62cd-48fc-aa21-7b97da30414b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time, datetime, random\n",
    "\n",
    "account = spark.conf.get(\"adls.account.name\")\n",
    "\n",
    "organization = 'FarmIA' #Carpta raíz en la que quedarán los ficheros\n",
    "landing_container = f\"abfss://landing-tarea@{account}.dfs.core.windows.net\"\n",
    "lakehouse_container = f\"abfss://lakehouse-tarea@{account}.dfs.core.windows.net\"\n",
    "\n",
    "landing_path = landing_container\n",
    "raw_path = f\"{lakehouse_container}/raw\"\n",
    "bronze_path = f\"{lakehouse_container}/bronze\"\n",
    "\n",
    "dbutils.fs.mkdirs(raw_path)\n",
    "dbutils.fs.mkdirs(bronze_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae692d20-0fd5-4d11-8c26-7a8e361080cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reutilizamos de los notebooks del módulo la función *land_file* que propone una convención de nombre con año, mes y día a cada archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5a936d-16b9-4d8e-9db7-a463f2aa5cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def land_file(df,datasource,dataset,format='json'):\n",
    "  \"\"\"\n",
    "    Guarda un DataFrame en un sistema de archivos distribuido con una estructura de directorios basada en la fecha actual,\n",
    "    utilizando un formato específico (por defecto, JSON). La función escribe el DataFrame en una ubicación temporal,\n",
    "    lo mueve a una ruta final organizada por fuente de datos, conjunto de datos y marca de tiempo, y luego elimina el\n",
    "    directorio temporal.\n",
    "\n",
    "    Parámetros:\n",
    "        df (pyspark.sql.DataFrame): El DataFrame de Spark que se desea guardar.\n",
    "        datasource (str): Nombre o identificador de la fuente de datos, usado para organizar la ruta final.\n",
    "        dataset (str): Nombre o identificador del conjunto de datos, usado para organizar la ruta final.\n",
    "        format (str, opcional): Formato en el que se guardará el archivo. Por defecto es 'json'. \n",
    "                                Otros formatos soportados dependen de Spark (e.g., 'parquet', 'csv').\n",
    "\n",
    "    Comportamiento:\n",
    "        1. Escribe el DataFrame en una carpeta temporal (`tmp_path`) usando el formato especificado, coalesciendo los datos en un solo archivo.\n",
    "        2. Genera una ruta final basada en la fecha actual (`YYYY/MM/DD`), el nombre de la fuente de datos, el conjunto de datos y una marca de tiempo.\n",
    "        3. Mueve el archivo generado desde la carpeta temporal a la ruta final.\n",
    "        4. Imprime la ruta final del archivo guardado.\n",
    "        5. Elimina la carpeta temporal.\n",
    "\n",
    "    Variables externas utilizadas:\n",
    "        - landing_path (str): Ruta base del sistema de archivos donde se almacenan los datos. Debe estar definida globalmente.\n",
    "        - dbutils.fs: Utilidad de Databricks para manipular el sistema de archivos (ls, mv, rm).\n",
    "        - datetime: Módulo de Python para manejar fechas y marcas de tiempo.\n",
    "\n",
    "    Ejemplo:\n",
    "        save_file(mi_dataframe, \"ventas\", \"diarias\", format=\"parquet\")\n",
    "        # Salida esperada: \"dbfs:/landing/ventas/diarias/2025/03/14/ventas_diarias_20250314123045.parquet\"\n",
    "\n",
    "    Notas:\n",
    "        - La función asume que está ejecutándose en un entorno compatible con Databricks (por el uso de `dbutils.fs`).\n",
    "        - Si el formato especificado no es compatible con Spark, se generará un error.\n",
    "    \"\"\"\n",
    "  tmp_path = f'{landing_path}/tmp/'\n",
    "  df.coalesce(1).write.format(format).mode(\"overwrite\").save(tmp_path)\n",
    "  now = datetime.datetime.utcnow()\n",
    "  date_path = now.strftime(\"%Y/%m/%d\")\n",
    "  timestamp = now.strftime(\"%Y%m%d%H%M%S\") \n",
    "  for file in dbutils.fs.ls(tmp_path):\n",
    "    if file.name.endswith(f'.{format}'):\n",
    "      final_path = file.path.replace('tmp',f'{datasource}/{dataset}')\n",
    "      final_path = final_path.replace(file.name, f'{date_path}/{datasource}-{dataset}-{timestamp}.{format}')\n",
    "      dbutils.fs.mv(file.path, final_path)\n",
    "      print(final_path)\n",
    "  dbutils.fs.rm(tmp_path, True)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc24c8c-e004-4754-b071-a314f1cd9ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reutilizaremos la función *LandingStreamReader* que admite múltiples formatos de ficheros, aplicando Auto Loader para leer de forma incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae83c90-8792-4b6d-b5e0-2d772462f6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name, replace,lit\n",
    "\n",
    "class LandingStreamReader:\n",
    "\n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "        self.format = builder.format\n",
    "        self.dataset_landing_path = f'{self.landing_path}/{self.datasource}/{self.dataset}'\n",
    "        self.dataset_bronze_schema_location = f'{self.bronze_path}/{self.datasource}/{self.dataset}_schema'\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_schema_location)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"LandingStreamReader(datasource='{self.datasource}',dataset='{self.dataset}')\")\n",
    "        \n",
    "    def add_metadata_columns(self,df):\n",
    "      data_cols = df.columns\n",
    "      \n",
    "      metadata_cols = ['_ingested_at','_ingested_filename']\n",
    "\n",
    "      df = (df.withColumn(\"_ingested_at\",current_timestamp())\n",
    "              .withColumn(\"_ingested_filename\",replace(input_file_name(),lit(self.landing_path),lit(self.raw_path)))\n",
    "      ) \n",
    "      \n",
    "      #reordernamos columnas\n",
    "      return df.select(metadata_cols + data_cols)  \n",
    "    \n",
    "    def read_json(self):\n",
    "      return (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "            .load(self.dataset_landing_path)\n",
    "      )\n",
    "\n",
    "    def read_csv(self):\n",
    "        return (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "            .option(\"header\",\"true\")\n",
    "            .option(\"delimiter\",\",\")\n",
    "            .load(self.dataset_landing_path)\n",
    "      )\n",
    "    \n",
    "    def read_parquet(self):\n",
    "       return (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"parquet\")\n",
    "            .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "            .load(self.dataset_landing_path)\n",
    "      ) \n",
    "    \n",
    "    def read_avro(self):\n",
    "        return (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"avro\")\n",
    "            .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "            .load(self.dataset_landing_path)\n",
    "      )\n",
    "    \n",
    "    def read_binaryfile(self):\n",
    "        return (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "            .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "            .load(self.dataset_landing_path)\n",
    "      )\n",
    "    \n",
    "    def read(self):\n",
    "      df = None\n",
    "\n",
    "      if (self.format == \"json\"):\n",
    "        df = self.read_json()\n",
    "      elif (self.format == \"csv\"):\n",
    "        df = self.read_csv()\n",
    "      elif (self.format == \"parquet\"):\n",
    "        df = self.read_parquet()\n",
    "      elif (self.format == \"avro\"):\n",
    "        df = self.read_avro()\n",
    "      elif self.format in (\"binaryFile\",\"binary\"):\n",
    "        df = self.read_binaryfile()\n",
    "      else:\n",
    "        raise Exception(f\"Format {self.format} not supported\")\n",
    "\n",
    "      if df is not None:\n",
    "        df = df.transform(self.add_metadata_columns)\n",
    "      return df\n",
    "    \n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "            self.format = None\n",
    "        \n",
    "        def set_datasource(self, datasource):\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "        \n",
    "        def set_dataset(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "        \n",
    "        def set_landing_path(self, landing_path):\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "        \n",
    "        def set_raw_path(self, raw_path):\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "        \n",
    "        def set_bronze_path(self, bronze_path):\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "          \n",
    "        def set_format(self, format):\n",
    "            self.format = format\n",
    "            return self\n",
    "        \n",
    "        def build(self):\n",
    "            return LandingStreamReader(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e000e4-ba36-4f3f-8f97-4ac59dd8385d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reutilizamos también la función *BronzeStreamWriter* que nos permite salvar el dataset en la capa Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74056b5-6ef0-43f8-ae32-4f5fc8647021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, current_timestamp\n",
    "\n",
    "class BronzeStreamWriter:\n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "        self.dataset_landing_path = f\"{self.landing_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_raw_path =  f\"{self.raw_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_bronze_path = f\"{self.bronze_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_checkpoint_location = f'{self.dataset_bronze_path}_checkpoint'\n",
    "        self.table = f'hive_metastore.bronze.{self.datasource}_{self.dataset}'\n",
    "        self.query_name = f\"bronze-{self.datasource}-{self.dataset}\"\n",
    "        dbutils.fs.mkdirs(self.dataset_raw_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_checkpoint_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"BronzeStreamWriter(datasource='{self.datasource}',dataset='{self.dataset}')\")\n",
    "         \n",
    "    def archive_raw_files(self,df):\n",
    "      if \"_ingested_filename\" in df.columns:\n",
    "        files = [row[\"_ingested_filename\"] for row in df.select(\"_ingested_filename\").distinct().collect()]\n",
    "        for file in files:\n",
    "          if file:\n",
    "              file_landing_path = file.replace(self.dataset_raw_path,self.dataset_landing_path)\n",
    "              dbutils.fs.mkdirs(file[0:file.rfind('/')+1])\n",
    "              dbutils.fs.mv(file_landing_path,file)\n",
    "    \n",
    "    def write_data(self,df):\n",
    "      spark.sql( 'CREATE DATABASE IF NOT EXISTS hive_metastore.bronze') \n",
    "      spark.sql(f\"CREATE TABLE IF NOT EXISTS {self.table} USING DELTA LOCATION '{self.dataset_bronze_path}' \") \n",
    "      (df.write\n",
    "          .format(\"delta\")  \n",
    "          .mode(\"append\")\n",
    "          .option(\"mergeSchema\", \"true\")\n",
    "          .option(\"path\", self.dataset_bronze_path)\n",
    "          .saveAsTable(self.table)\n",
    "      )\n",
    "        \n",
    "    def append_2_bronze(self,batch_df, batch_id):\n",
    "      batch_df.persist()\n",
    "      self.write_data(batch_df)\n",
    "      self.archive_raw_files(batch_df)\n",
    "      batch_df.unpersist()\n",
    "      \n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "        \n",
    "        def set_datasource(self, datasource):\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "        \n",
    "        def set_dataset(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "        \n",
    "        def set_landing_path(self, landing_path):\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "        \n",
    "        def set_raw_path(self, raw_path):\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "        \n",
    "        def set_bronze_path(self, bronze_path):\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "        \n",
    "        def build(self):\n",
    "            return BronzeStreamWriter(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c64a2c7-8db5-402f-a305-c97a78e59846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A continuación generamos los datos de ejemplo en la capa landing.\\ Utilizaremos el dataset propio de Databricks *retail-org/sales_orders*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72692416-2bdb-4cb9-aabb-70c8e86dd935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dataset de ventas de clientes \n",
    "datasource=\"retail-org\"\n",
    "dataset=\"sales_orders\"\n",
    "\n",
    "dataset_bronze_path = f\"{bronze_path}/{datasource}/{dataset}\"\n",
    "\n",
    "df_sales = spark.read.json(f\"/databricks-datasets/{datasource}/{dataset}/\")\n",
    "#df_sales.printSchema()\n",
    "\n",
    "\n",
    "file_sales_1 = df_sales.where(\"customer_id <= 10000000\").coalesce(1)\n",
    "file_sales_2 = df_sales.where(\"customer_id between 10000000 and 20000000\").coalesce(1)\n",
    "file_sales_3 = df_sales.where(\"customer_id > 20000000\").coalesce(1)\n",
    "\n",
    "land_file(file_sales_1,'FarmIA','sales')\n",
    "time.sleep(5)\n",
    "land_file(file_sales_2,'FarmIA','sales')\n",
    "time.sleep(5)\n",
    "land_file(file_sales_3,'FarmIA','sales')\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e732cb3-7dfb-4df9-a1de-50def19ea173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{landing_path}/FarmIA/sales/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1939f7fd-e780-472a-802f-1c55b42ec4b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Utilizaremos ahora Auto Loader, que detectará automáticamente los ficheros generados enteriormente y los moverá a la capa Bronze con metadatos y evolución del esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b136521-a4dd-43aa-83ff-d1aa2740303e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datasource=\"FarmIA\"\n",
    "dataset = \"sales\"\n",
    "\n",
    "dataset_bronze_path = f\"{bronze_path}/{datasource}/{dataset}\"\n",
    "\n",
    "reader = (LandingStreamReader.Builder()\n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  .set_landing_path(landing_path)\n",
    "  .set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .set_format(\"json\")   # o csv/parquet/avro/binaryFile\n",
    "  .build()\n",
    ")\n",
    "\n",
    "writer = (BronzeStreamWriter.Builder()\n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  .set_landing_path(landing_path)\n",
    "  .set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "(reader.read()\n",
    " .writeStream\n",
    " .foreachBatch(writer.append_2_bronze)\n",
    " .trigger(availableNow = True)\n",
    " .option(\"checkpointLocation\", writer.dataset_checkpoint_location)\n",
    " .queryName(writer.query_name)\n",
    " .start()\n",
    " .awaitTermination()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d0003e-0a93-4166-b253-b2dedac861f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce87d12-1da2-478c-8cbd-803d73c63537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingesta_datos_batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
